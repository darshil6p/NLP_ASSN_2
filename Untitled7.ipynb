{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPdKTeiTQ42/7bHuTEu+iTY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshil6p/NLP_ASSN_2/blob/master/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6mouesVfmAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dense,Dropout,Flatten\n",
        "from keras.layers import Activation, Conv1D, GlobalMaxPooling1D\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8PaFFs3kOMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the train/test split package from sklearn for preparing our dataset to\n",
        "# train and test the model with\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "#Import train data into pandas dataframe\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv', sep='\\t')\n",
        "#data.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiu7e27Jk7Tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Select colums for sentences\n",
        "SentDf = pd.DataFrame(data,columns=['Phrase', 'Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE5awjTPlDKS",
        "colab_type": "code",
        "outputId": "c9e9aaa6-5bac-4a22-cc56-3312b45372c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Using nltk to tokenize data\n",
        "documents = []\n",
        "for i in range(SentDf.shape[0]):\n",
        "  tmpWords = word_tokenize(SentDf['Phrase'][i])\n",
        "  documents.append((tmpWords, SentDf['Sentiment'][i]))\n",
        "random.seed(9001)\n",
        "random.shuffle(documents)\n",
        "print(documents[1][0])\n",
        "len(documents)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['can', 'only', 'remind', 'us', 'of', 'brilliant', 'crime', 'dramas', 'without', 'becoming', 'one', 'itself', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "156060"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezOU42i3lv7r",
        "colab_type": "code",
        "outputId": "bf45ec94-d925-4e08-b6d0-afae822c63e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Remove Punctuations and lemmatize it\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "punctuations=\"?:!.,;'\\\"-()\"\n",
        "\n",
        "#parameters to adjust to see the impact on outcome\n",
        "useStemming = True\n",
        "removePuncs = True\n",
        "\n",
        "for l in range(len(documents)):\n",
        "  label = documents[l][1]\n",
        "  tmpReview = []\n",
        "  for w in documents[l][0]:\n",
        "    newWord = w\n",
        "    if removePuncs and (w in punctuations):\n",
        "      continue\n",
        "    if useStemming:\n",
        "      newWord = lancaster.stem(newWord)\n",
        "    tmpReview.append(newWord)\n",
        "  documents[l] = (' '.join(tmpReview), label)\n",
        "print(documents[2])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "(\"walk out mut word lik `` horr '' and `` terr '' but\", 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9l3e7nLl08i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = pd.DataFrame(documents,columns=['text', 'sentiment'])\n",
        "\n",
        "# Splits the dataset so 70% is used for training and 30% for testing\n",
        "x_train_raw, x_test_raw, Y_train, Y_test = train_test_split(\n",
        "    all_data['text'], all_data['sentiment'], test_size=0.3,random_state=2003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUR5Z302pig3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#Constraining vector length to avoid memory error\n",
        "vec_len = 12500\n",
        "# Transform each text into a vector of word counts\n",
        "vectorizer = CountVectorizer(max_features=vec_len, vocabulary=None, \n",
        "                                 lowercase=True,\n",
        "                                 strip_accents='unicode',\n",
        "                                 stop_words=set(stopwords.words('english')),\n",
        "                                 ngram_range=(1,4),\n",
        "                                 analyzer='word',\n",
        "                                 min_df=5,\n",
        "                                 max_df=.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdffFRMnmFOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = to_categorical(Y_train)\n",
        "Y_train.shape\n",
        "#Shape data to fit in our model\n",
        "train_bow = vectorizer.fit_transform(x_train_raw)\n",
        "test_bow = vectorizer.transform(x_test_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au0VQTaQk3fL",
        "colab_type": "code",
        "outputId": "218c114a-cfc0-42bf-b6a7-ce1a81e164d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "#Defining the cnn model\n",
        "def baseline_cnn_model(fea_matrix, n_class, mode, compiler):\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=64, kernel_size=1, activation='tanh',\n",
        "                  input_shape=(fea_matrix.shape[1],fea_matrix.shape[2])))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Conv1D(filters=128, kernel_size=1, activation='tanh'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Activation('tanh'))\n",
        "  model.add(Dense(n_class))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(optimizer=compiler,loss='categorical_crossentropy',\n",
        "                 metrics=['acc',f1_m,precision_m,recall_m])\n",
        "  return model\n",
        "\n",
        "try:\n",
        "  train_bow = np.array(train_bow.toarray())\n",
        "  train_bow = train_bow.reshape(train_bow.shape[0], train_bow.shape[1], 1)\n",
        "  test_bow = np.array(test_bow.toarray())\n",
        "  test_bow = test_bow.reshape(test_bow.shape[0], test_bow.shape[1], 1)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "train_bow.shape\n",
        "\n",
        "adm = optimizers.Adam(lr=1e-3,decay=1e-4)\n",
        "model = baseline_cnn_model(train_bow,5,'reg',adm)\n",
        "y_test_binary = to_categorical(Y_test)\n",
        "\n",
        "model1=model.fit(train_bow,Y_train,batch_size=128,\n",
        "         epochs=25,verbose=1,validation_split=0.3)\n",
        "\n",
        "#Function for printing the metrics\n",
        "def print_metrics(accuracy, f1_score,precision,recall):\n",
        "  print('\\n')\n",
        "  print('Accuracy  : ',np.round(accuracy, 4))\n",
        "  print('Precision : ',np.round(precision, 4))\n",
        "  print('Recall    : ',np.round(recall, 4))\n",
        "  print('F1 Score  : ',np.round(f1_score, 4))\n",
        "  print('\\n')\n",
        "  \n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(test_bow,y_test_binary)\n",
        "print_metrics(accuracy, f1_score,precision,recall)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 76469 samples, validate on 32773 samples\n",
            "Epoch 1/25\n",
            "76469/76469 [==============================] - 51s 670us/step - loss: 1.1200 - acc: 0.5624 - f1_m: 0.4986 - precision_m: 0.6173 - recall_m: 0.4270 - val_loss: 1.0517 - val_acc: 0.5862 - val_f1_m: 0.5491 - val_precision_m: 0.6666 - val_recall_m: 0.4675\n",
            "Epoch 2/25\n",
            "76469/76469 [==============================] - 51s 665us/step - loss: 0.9985 - acc: 0.6058 - f1_m: 0.5736 - precision_m: 0.6660 - recall_m: 0.5048 - val_loss: 1.0296 - val_acc: 0.5990 - val_f1_m: 0.5715 - val_precision_m: 0.6582 - val_recall_m: 0.5054\n",
            "Epoch 3/25\n",
            "76469/76469 [==============================] - 51s 666us/step - loss: 0.9630 - acc: 0.6177 - f1_m: 0.5896 - precision_m: 0.6753 - recall_m: 0.5242 - val_loss: 1.0488 - val_acc: 0.6021 - val_f1_m: 0.5778 - val_precision_m: 0.6509 - val_recall_m: 0.5199\n",
            "Epoch 4/25\n",
            "76469/76469 [==============================] - 51s 666us/step - loss: 0.9432 - acc: 0.6249 - f1_m: 0.5984 - precision_m: 0.6803 - recall_m: 0.5350 - val_loss: 1.0371 - val_acc: 0.6010 - val_f1_m: 0.5756 - val_precision_m: 0.6544 - val_recall_m: 0.5142\n",
            "Epoch 5/25\n",
            "76469/76469 [==============================] - 51s 666us/step - loss: 0.9258 - acc: 0.6310 - f1_m: 0.6052 - precision_m: 0.6841 - recall_m: 0.5434 - val_loss: 1.0379 - val_acc: 0.5995 - val_f1_m: 0.5809 - val_precision_m: 0.6468 - val_recall_m: 0.5275\n",
            "Epoch 6/25\n",
            "76469/76469 [==============================] - 51s 666us/step - loss: 0.9151 - acc: 0.6354 - f1_m: 0.6103 - precision_m: 0.6886 - recall_m: 0.5488 - val_loss: 1.0477 - val_acc: 0.6018 - val_f1_m: 0.5709 - val_precision_m: 0.6534 - val_recall_m: 0.5073\n",
            "Epoch 7/25\n",
            "76469/76469 [==============================] - 51s 666us/step - loss: 0.9069 - acc: 0.6377 - f1_m: 0.6119 - precision_m: 0.6878 - recall_m: 0.5518 - val_loss: 1.0440 - val_acc: 0.5982 - val_f1_m: 0.5766 - val_precision_m: 0.6474 - val_recall_m: 0.5202\n",
            "Epoch 8/25\n",
            "76469/76469 [==============================] - 51s 667us/step - loss: 0.8994 - acc: 0.6403 - f1_m: 0.6170 - precision_m: 0.6918 - recall_m: 0.5575 - val_loss: 1.0535 - val_acc: 0.6009 - val_f1_m: 0.5622 - val_precision_m: 0.6590 - val_recall_m: 0.4907\n",
            "Epoch 9/25\n",
            "76469/76469 [==============================] - 51s 665us/step - loss: 0.8929 - acc: 0.6425 - f1_m: 0.6193 - precision_m: 0.6941 - recall_m: 0.5597 - val_loss: 1.0437 - val_acc: 0.6024 - val_f1_m: 0.5794 - val_precision_m: 0.6482 - val_recall_m: 0.5243\n",
            "Epoch 10/25\n",
            "76469/76469 [==============================] - 51s 665us/step - loss: 0.8871 - acc: 0.6441 - f1_m: 0.6214 - precision_m: 0.6945 - recall_m: 0.5629 - val_loss: 1.0588 - val_acc: 0.5994 - val_f1_m: 0.5818 - val_precision_m: 0.6416 - val_recall_m: 0.5326\n",
            "Epoch 11/25\n",
            "76469/76469 [==============================] - 51s 665us/step - loss: 0.8828 - acc: 0.6456 - f1_m: 0.6242 - precision_m: 0.6968 - recall_m: 0.5659 - val_loss: 1.0525 - val_acc: 0.6009 - val_f1_m: 0.5826 - val_precision_m: 0.6431 - val_recall_m: 0.5329\n",
            "Epoch 12/25\n",
            "76469/76469 [==============================] - 51s 665us/step - loss: 0.8801 - acc: 0.6472 - f1_m: 0.6258 - precision_m: 0.6977 - recall_m: 0.5680 - val_loss: 1.0526 - val_acc: 0.6034 - val_f1_m: 0.5792 - val_precision_m: 0.6515 - val_recall_m: 0.5218\n",
            "Epoch 13/25\n",
            "76469/76469 [==============================] - 51s 665us/step - loss: 0.8757 - acc: 0.6480 - f1_m: 0.6252 - precision_m: 0.6977 - recall_m: 0.5669 - val_loss: 1.0581 - val_acc: 0.6040 - val_f1_m: 0.5781 - val_precision_m: 0.6495 - val_recall_m: 0.5212\n",
            "Epoch 14/25\n",
            "76469/76469 [==============================] - 51s 667us/step - loss: 0.8724 - acc: 0.6491 - f1_m: 0.6281 - precision_m: 0.6996 - recall_m: 0.5705 - val_loss: 1.0597 - val_acc: 0.5999 - val_f1_m: 0.5822 - val_precision_m: 0.6401 - val_recall_m: 0.5342\n",
            "Epoch 15/25\n",
            "76469/76469 [==============================] - 51s 664us/step - loss: 0.8685 - acc: 0.6514 - f1_m: 0.6312 - precision_m: 0.7013 - recall_m: 0.5745 - val_loss: 1.0547 - val_acc: 0.6016 - val_f1_m: 0.5787 - val_precision_m: 0.6483 - val_recall_m: 0.5230\n",
            "Epoch 16/25\n",
            "76469/76469 [==============================] - 51s 665us/step - loss: 0.8655 - acc: 0.6516 - f1_m: 0.6304 - precision_m: 0.7012 - recall_m: 0.5732 - val_loss: 1.0595 - val_acc: 0.6029 - val_f1_m: 0.5767 - val_precision_m: 0.6494 - val_recall_m: 0.5190\n",
            "Epoch 17/25\n",
            "76469/76469 [==============================] - 51s 665us/step - loss: 0.8636 - acc: 0.6532 - f1_m: 0.6322 - precision_m: 0.7020 - recall_m: 0.5755 - val_loss: 1.0654 - val_acc: 0.6041 - val_f1_m: 0.5776 - val_precision_m: 0.6475 - val_recall_m: 0.5217\n",
            "Epoch 18/25\n",
            "76469/76469 [==============================] - 51s 664us/step - loss: 0.8615 - acc: 0.6545 - f1_m: 0.6331 - precision_m: 0.7028 - recall_m: 0.5765 - val_loss: 1.0655 - val_acc: 0.6026 - val_f1_m: 0.5757 - val_precision_m: 0.6493 - val_recall_m: 0.5176\n",
            "Epoch 19/25\n",
            "76469/76469 [==============================] - 51s 665us/step - loss: 0.8588 - acc: 0.6537 - f1_m: 0.6334 - precision_m: 0.7040 - recall_m: 0.5761 - val_loss: 1.0704 - val_acc: 0.6040 - val_f1_m: 0.5806 - val_precision_m: 0.6466 - val_recall_m: 0.5273\n",
            "Epoch 20/25\n",
            "76469/76469 [==============================] - 51s 665us/step - loss: 0.8578 - acc: 0.6539 - f1_m: 0.6341 - precision_m: 0.7032 - recall_m: 0.5778 - val_loss: 1.0651 - val_acc: 0.6043 - val_f1_m: 0.5789 - val_precision_m: 0.6486 - val_recall_m: 0.5231\n",
            "Epoch 21/25\n",
            "76469/76469 [==============================] - 52s 675us/step - loss: 0.8558 - acc: 0.6547 - f1_m: 0.6350 - precision_m: 0.7043 - recall_m: 0.5787 - val_loss: 1.0700 - val_acc: 0.6018 - val_f1_m: 0.5802 - val_precision_m: 0.6450 - val_recall_m: 0.5276\n",
            "Epoch 22/25\n",
            "76469/76469 [==============================] - 52s 676us/step - loss: 0.8540 - acc: 0.6557 - f1_m: 0.6362 - precision_m: 0.7048 - recall_m: 0.5804 - val_loss: 1.0827 - val_acc: 0.6004 - val_f1_m: 0.5867 - val_precision_m: 0.6350 - val_recall_m: 0.5456\n",
            "Epoch 23/25\n",
            "76469/76469 [==============================] - 52s 676us/step - loss: 0.8528 - acc: 0.6559 - f1_m: 0.6371 - precision_m: 0.7063 - recall_m: 0.5808 - val_loss: 1.0779 - val_acc: 0.6019 - val_f1_m: 0.5767 - val_precision_m: 0.6474 - val_recall_m: 0.5204\n",
            "Epoch 24/25\n",
            "76469/76469 [==============================] - 52s 677us/step - loss: 0.8510 - acc: 0.6582 - f1_m: 0.6376 - precision_m: 0.7057 - recall_m: 0.5821 - val_loss: 1.0754 - val_acc: 0.6028 - val_f1_m: 0.5774 - val_precision_m: 0.6449 - val_recall_m: 0.5231\n",
            "Epoch 25/25\n",
            "76469/76469 [==============================] - 52s 676us/step - loss: 0.8498 - acc: 0.6583 - f1_m: 0.6390 - precision_m: 0.7077 - recall_m: 0.5830 - val_loss: 1.0773 - val_acc: 0.6003 - val_f1_m: 0.5822 - val_precision_m: 0.6416 - val_recall_m: 0.5332\n",
            "46818/46818 [==============================] - 15s 319us/step\n",
            "\n",
            "\n",
            "Accuracy  :  0.6031\n",
            "Precision :  0.6437\n",
            "Recall    :  0.534\n",
            "F1 Score  :  0.5827\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EWUNr28F38_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b8d80c4-3b85-4bc3-d839-a6a9a6b476a3"
      },
      "source": [
        "model.save(\"1105933_1dconv_reg.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq1W-cjSFwG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}